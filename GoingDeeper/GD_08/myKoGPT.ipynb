{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b96ba81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:1.12.1\n",
      "Cuda version: 11.3\n",
      "transformers version: 4.28.0\n",
      "GPU 사용 가능여부: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n",
    "print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n",
    "print(\"transformers version: {}\".format(transformers.__version__)) # transformers 4.28.0\n",
    "print(\"GPU 사용 가능여부: {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a112380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "     |████████████████████████████████| 84 kB 2.1 MB/s            \n",
      "\u001b[?25hCollecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.26.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.3.3)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.15.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.13.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.21.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2021.11.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.62.3)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge_score) (0.12.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (12.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->evaluate) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2021.11.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2021.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=c13aea3f2d641e85cf0401db69a79089989eaeb08b554251852b6c28972f48d4\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score, evaluate\n",
      "Successfully installed evaluate-0.4.3 rouge-score-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3867058",
   "metadata": {},
   "source": [
    "### 모델 불러오기 - skt/kogpt2-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9131bdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a701409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': 1024,\n",
       " 'gpt2-medium': 1024,\n",
       " 'gpt2-large': 1024,\n",
       " 'gpt2-xl': 1024,\n",
       " 'distilgpt2': 1024}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a896df67",
   "metadata": {},
   "source": [
    "### SFT 데이터 - 기본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aacb4fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '불고기용 고기 한우에요?',\n",
       "  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
       "  'tokens': 193},\n",
       " {'prompt': '쓰던 앱이 유료로 전환됐어',\n",
       "  'completion': \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\",\n",
       "  'tokens': 288},\n",
       " {'prompt': '여친이랑 다툼',\n",
       "  'completion': \"'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지만 다투게 되면 상대방의 말을 경청하는 태도와 서로의 감정을 존중하는 것이 중요합니다. 논리적이고 차분한 대화를 통해 문제를 해결해보세요. 그리고 서로를 이해하고 서로의 의견을 수용하는 것이 중요합니다.\",\n",
       "  'tokens': 153}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "data_path_1_SFT = './data_kochatgpt/kochatgpt_1_SFT.jsonl' \n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a36bb2",
   "metadata": {},
   "source": [
    "### RM 데이터 - 기본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03fd6cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?',\n",
       "  'completion_0': 'Allow me to answer your question. I know that you are curious about me.',\n",
       "  'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.',\n",
       "  'completion_2': '라이언에게 말했다.',\n",
       "  'ranking': [2, 1, 0]},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?',\n",
       "  'completion_0': '개포주공아파트는 다섯 단지로 이루어져 있습니다.',\n",
       "  'completion_1': '이날 목송에서 구글상위노',\n",
       "  'completion_2': '개포주공아파트는 총 27개 단지로 이루어져 있습니다.',\n",
       "  'ranking': [2, 0, 1]},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?',\n",
       "  'completion_0': 'The diameter of the Metallic domain is bigger than the Hyperonic domain.',\n",
       "  'completion_1': '이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가 그 발언을 문제삼았는지에 따라 답이 다를 수 있습니다.\\\\n\\\\n만약 김영삼 대통령이 후보 시절에 지역표심을 겨냥한 발언을 했다는 가정하에, 그 발언을 문제삼은 후보가 누구였는지를 대답하자면, 그 답은 이화선 당시 민주당 대통령 후보가 될 것입니다. 1992년 총선 때, 김영삼 대선후보는 \"집값이 오른 노량진역 부근의 부동산 가격은 세월호 폭침 후 \\\\\\'강남 도시재생\\\\\\' 일환으로 상승했다\"는 발언을 했습니다. 하지만 이화선 후보는 이 발언을 \"전국적으로 경제적 발전이 이루어지지 않은 지방민의 마음을 멀리해지려는 무례한 발언\"이라고 비판하며 문제삼았습니다.\\\\n\\\\n하지만, 이 질문을 답변하는 데 있어서 보다 명확한 정보가 있으면 답변을 보완할 수 있습니다.',\n",
       "  'completion_2': '김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 추구하고 있는 민주주의 광범위하게 확립과 보수의 사상을 이어가는 데 있어 지역경제 발전과 공공서비스 신속 개선을 위해 합리적인 국가 정책에 따르는 방향성을 제시하고 있습니다.',\n",
       "  'ranking': [1, 2, 0]}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_2_RM = './data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "with open(data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb470a6",
   "metadata": {},
   "source": [
    "### PPO 데이터 - 기본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d90f9dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?'},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?'},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_3_PPO = './data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2847a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa2334c",
   "metadata": {},
   "source": [
    "### SFT 모델 토크나이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a6f28a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303c2fb",
   "metadata": {},
   "source": [
    "### SFT 데이터셋 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7159ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da39221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2278118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT='./data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e711d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/KoChatGPT/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f5250ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 09:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.947900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.759300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.742400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.656700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('/KoChatGPT/output_1_SFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53e71d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer # rouge_score 라이브러리 임포트\n",
    "\n",
    "def calculate_and_print_rouge_scores(generated_texts: list,\n",
    "                                     reference_texts: list,\n",
    "                                     use_stemmer: bool = False,\n",
    "                                     print_individual_scores: bool = True):\n",
    "    \"\"\"\n",
    "    생성된 텍스트와 참조 텍스트 간의 ROUGE 점수를 계산하고 결과를 출력합니다.\n",
    "\n",
    "    Args:\n",
    "        generated_texts (list): 모델이 생성한 텍스트(응답)의 리스트입니다.\n",
    "        reference_texts (list): 참조 (정답) 텍스트의 리스트입니다.\n",
    "        use_stemmer (bool): ROUGE 계산 시 형태소 분석기(stemmer) 사용 여부입니다.\n",
    "                            True로 설정 시, NLTK 등의 라이브러리가 필요할 수 있으며,\n",
    "                            한국어의 경우 적절한 한국어 형태소 분석기 설정이 필요합니다.\n",
    "                            기본값은 False입니다.\n",
    "        print_individual_scores (bool): 각 샘플별 ROUGE 점수 출력 여부입니다. 기본값은 True입니다.\n",
    "\n",
    "    Returns:\n",
    "        dict: 평균 ROUGE Precision, Recall, F1-score를 담은 딕셔너리입니다.\n",
    "              (예: {'rouge1_precision': 0.XX, 'rouge1_recall': 0.YY, 'rouge1_f1': 0.ZZ, ...})\n",
    "              텍스트 목록이 비어 있거나 길이가 다를 경우 None을 반환합니다.\n",
    "    \"\"\"\n",
    "    if not generated_texts or not reference_texts:\n",
    "        print(\"🚫 생성된 텍스트 또는 참조 텍스트 목록이 비어있습니다. ROUGE 점수를 계산할 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    if len(generated_texts) != len(reference_texts):\n",
    "        print(f\"🚫 생성된 텍스트의 수({len(generated_texts)})와 참조 텍스트의 수({len(reference_texts)})가 일치하지 않아 ROUGE 점수를 계산할 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # rouge_scorer 초기화\n",
    "        # 지원되는 메트릭: 'rouge1', 'rouge2', ..., 'rougeL', 'rougeLsum'\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=use_stemmer)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ RougeScorer 초기화 중 오류 발생: {e}\")\n",
    "        print(\"ROUGE 점수 계산을 진행할 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    # 모든 샘플의 ROUGE 점수를 저장할 딕셔너리\n",
    "    all_scores_p = {'rouge1': [], 'rouge2': [], 'rougeL': []} # Precision\n",
    "    all_scores_r = {'rouge1': [], 'rouge2': [], 'rougeL': []} # Recall\n",
    "    all_scores_f = {'rouge1': [], 'rouge2': [], 'rougeL': []} # F1-score\n",
    "\n",
    "    if print_individual_scores:\n",
    "        print(\"\\n\\n--- 💯 개별 샘플 ROUGE 스코어 ---\")\n",
    "\n",
    "    for i in range(len(generated_texts)):\n",
    "        candidate = str(generated_texts[i])  # 생성된 텍스트\n",
    "        reference = str(reference_texts[i])  # 참조 텍스트\n",
    "\n",
    "        # ROUGE 점수 계산\n",
    "        # scores는 각 rouge 타입 (예: 'rouge1')에 대해 Score(precision=..., recall=..., fmeasure=...) 객체를 포함하는 딕셔너리\n",
    "        scores = scorer.score(reference, candidate)\n",
    "\n",
    "        if print_individual_scores:\n",
    "            print(f\"\\n📜 샘플 {i+1}:\")\n",
    "            # print(f\"  참조 답변: {reference}\") # 필요시 주석 해제하여 확인\n",
    "            # print(f\"  생성된 답변: {candidate}\") # 필요시 주석 해제하여 확인\n",
    "            print(f\"  ROUGE 점수:\")\n",
    "            for rouge_type, score_obj in scores.items():\n",
    "                print(f\"    {rouge_type.upper()}: P={score_obj.precision:.4f}, R={score_obj.recall:.4f}, F1={score_obj.fmeasure:.4f}\")\n",
    "\n",
    "        # 각 타입별 점수 저장\n",
    "        for rouge_type in ['rouge1', 'rouge2', 'rougeL']:\n",
    "            all_scores_p[rouge_type].append(scores[rouge_type].precision)\n",
    "            all_scores_r[rouge_type].append(scores[rouge_type].recall)\n",
    "            all_scores_f[rouge_type].append(scores[rouge_type].fmeasure)\n",
    "\n",
    "    # 평균 ROUGE 점수 계산\n",
    "    average_results = {}\n",
    "    if all_scores_f['rouge1']:  # 점수가 하나라도 계산되었다면\n",
    "        print(\"\\n\\n--- 📉 전체 샘플 평균 ROUGE 스코어 ---\")\n",
    "        for rouge_type in ['rouge1', 'rouge2', 'rougeL']:\n",
    "            avg_p = sum(all_scores_p[rouge_type]) / len(all_scores_p[rouge_type])\n",
    "            avg_r = sum(all_scores_r[rouge_type]) / len(all_scores_r[rouge_type])\n",
    "            avg_f = sum(all_scores_f[rouge_type]) / len(all_scores_f[rouge_type])\n",
    "\n",
    "            average_results[f'{rouge_type}_precision'] = avg_p\n",
    "            average_results[f'{rouge_type}_recall'] = avg_r\n",
    "            average_results[f'{rouge_type}_f1'] = avg_f\n",
    "            \n",
    "            print(f\"  평균 {rouge_type.upper()}: P={avg_p:.4f}, R={avg_r:.4f}, F1={avg_f:.4f}\")\n",
    "        \n",
    "        return average_results\n",
    "    else:\n",
    "        print(\"⚠️ 계산된 ROUGE 점수가 없어 평균을 반환할 수 없습니다.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "174f1269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 고기를 먹을 수 없습니다. 하지만 일반적으로 불고기는 건강에 좋은 식품 중 하나입니다. 쇠고기의 고기는 단백질, 지방, 철분, 칼슘, 철분, 인 등이 풍부하기 때문에 건강에 좋습니다. 또한, 소고기와 돼지고기를 섞어 만든 양념도 인기 있는 요리 중 하나입니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 42대 부통령직을 수행했습니다. 리처드 닉슨은 46대 부통령직을 맡았습니다. 리처드 닉슨은 48대 부통령을 역임하였습니다. 리처드는 40대 부통령을 맡았던 적이 있습니다. 리처드는 47대 부통령을 맡은 적이 없습니다. 리처드의 재임 기간 동안, 리처드 닉슨은\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'시카고 오 헤어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다. American International Pacific Language model, Translation of the Korean Capilities in Canada Orientality and Distributed Commissions.\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이므로 미세먼지 정보를 알 수 없습니다. 하지만, 미세먼지 예보나 예보 사이트를 통해 미세먼지 농도를 확인하실 수 있으니 참고하시기 바랍니다. 감사합니다. Please provide model, I do not have\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='/KoChatGPT/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a21e0",
   "metadata": {},
   "source": [
    "## 정성적 평가\n",
    "\n",
    "전체적으로 괜찮은 대답을 하고 있다는 생각이 든다.\n",
    "\n",
    "다만 중간에 대답이 영어로 바뀌는 부분을 다수 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cfdc17",
   "metadata": {},
   "source": [
    "## 제미나이가 생성한 reference와 rouge 스코어 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e0073ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 AI 어시스턴트이기 때문에 고기를 먹을 수 없습니다. 하지만 일반적으로 불고기는 건강하고 맛있을 뿐만 아니라 다양한 요리에 활용될 수 있기 때문에 많은 사람들이 즐겨먹는 음식 중 하나입니다. 高僧, 後僧, 五僧, 五乘, 五乘 등이 있습니다. 高僧은\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 41대 부통령직을 수행하였습니다. 리처드 닉슨은 46대 부통령직을 맡았습니다. 리처드 닉슨은 36대 부통령직을 맡았던 적이 있습니다. 리처드 닉슨이 38대 부통령직을 맡은 연도는 정확히 알려져 있지 않습니다. 리처드 닉슨은 43대 부통령을 역임했습니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'시카고 오 헤어 국제공항은 미국 텍사스주 휴스턴에 위치해 있습니다. Island of the translation of the Translation:\\n\\n시카고오 헤어 국제공항(The British Heautiful Metropolica) 지역입니다. Island\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이기 때문에 미세먼지 정보를 알 수 없습니다. 하지만 일반적으로 미세먼지는 대부분 중국에서 발원하여 국내로 유입됩니다. 따라서 외출 전에 마스크를 착용하시는 것이 좋습니다. 또한, 미세먼지 농도가 높은 날에는 실외 활동을 자제하는 것이 좋습니다.)孔三郎\n",
      "\n",
      "Extracting generated responses...\n",
      "\n",
      "✨ ROUGE 스코어 함수를 호출하여 모델 성능 평가를 시작합니다...\n",
      "\n",
      "\n",
      "--- 💯 개별 샘플 ROUGE 스코어 ---\n",
      "\n",
      "📜 샘플 1:\n",
      "  ROUGE 점수:\n",
      "    ROUGE1: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGE2: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGEL: P=0.0000, R=0.0000, F1=0.0000\n",
      "\n",
      "📜 샘플 2:\n",
      "  ROUGE 점수:\n",
      "    ROUGE1: P=0.2000, R=0.3333, F1=0.2500\n",
      "    ROUGE2: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGEL: P=0.2000, R=0.3333, F1=0.2500\n",
      "\n",
      "📜 샘플 3:\n",
      "  ROUGE 점수:\n",
      "    ROUGE1: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGE2: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGEL: P=0.0000, R=0.0000, F1=0.0000\n",
      "\n",
      "📜 샘플 4:\n",
      "  ROUGE 점수:\n",
      "    ROUGE1: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGE2: P=0.0000, R=0.0000, F1=0.0000\n",
      "    ROUGEL: P=0.0000, R=0.0000, F1=0.0000\n",
      "\n",
      "\n",
      "--- 📉 전체 샘플 평균 ROUGE 스코어 ---\n",
      "  평균 ROUGE1: P=0.0500, R=0.0833, F1=0.0625\n",
      "  평균 ROUGE2: P=0.0000, R=0.0000, F1=0.0000\n",
      "  평균 ROUGEL: P=0.0500, R=0.0833, F1=0.0625\n",
      "\n",
      "✅ ROUGE 점수 계산이 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "# (제공해주신 코드 부분)\n",
    "# generator = pipeline(...)\n",
    "# generation_args = dict(...)\n",
    "# PROMPT_DICT = {...}\n",
    "# list_prompt = [...] # 포맷팅된 프롬프트\n",
    "# list_result = generator(list_prompt, **generation_args)\n",
    "# for prompt, result in zip(list_prompt, list_result):\n",
    "#     print()\n",
    "#     print((result[0]['generated_text']))\n",
    "\n",
    "generator = pipeline('text-generation', model='/KoChatGPT/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))\n",
    "\n",
    "# --- 👇 ROUGE 함수 호출을 위한 추가 코드 ---\n",
    "\n",
    "# 1. 모델이 생성한 \"응답\" 부분만 추출하기\n",
    "list_generated_responses = []\n",
    "response_marker = PROMPT_DICT[\"prompt_input\"].split(\"{prompt}\")[1].split(\"### Response(응답):\")[0] + \"### Response(응답):\" # \"### Response(응답):\" 마커\n",
    "\n",
    "# 원본 질문 (참조 텍스트와 매칭하기 위함)\n",
    "list_prompt_original = ['불고기용 고기 한우에요?',\n",
    "                       '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "                       '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "                       '오늘 미세먼지 어때?']\n",
    "\n",
    "print(\"\\nExtracting generated responses...\")\n",
    "for i, single_result_list in enumerate(list_result):\n",
    "    full_generated_text = single_result_list[0]['generated_text']\n",
    "    \n",
    "    # \"### Response(응답):\" 마커 이후의 텍스트를 추출\n",
    "    # rfind를 사용하여 프롬프트와 응답 사이에 마커가 여러 번 나오는 경우, 마지막 마커를 기준으로 함\n",
    "    marker_position = full_generated_text.rfind(response_marker)\n",
    "    \n",
    "    if marker_position != -1:\n",
    "        # 마커 다음부터 텍스트 추출\n",
    "        response_only = full_generated_text[marker_position + len(response_marker):].strip()\n",
    "    else:\n",
    "        # 마커를 찾지 못한 경우, 포맷팅된 프롬프트(list_prompt[i]) 이후의 텍스트를 가져오려는 시도\n",
    "        # 이 방법은 모델이 프롬프트를 그대로 반복하지 않으면 정확하지 않을 수 있습니다.\n",
    "        try:\n",
    "            response_only = full_generated_text.split(list_prompt[i])[1].strip()\n",
    "        except IndexError:\n",
    "            print(f\"⚠️ Warning: 응답 마커 및 프롬프트 분리를 찾지 못했습니다. 샘플 {i}는 전체 텍스트를 사용합니다.\")\n",
    "            response_only = full_generated_text # 최후의 수단 (개선 필요)\n",
    "\n",
    "    # eos_token (예: '\\n' 또는 tokenizer.eos_token) 정리\n",
    "    # generation_args의 eos_token_id가 375 ('\\n')로 되어 있으므로, 이를 기준으로 처리\n",
    "    # tokenizer.eos_token (예: '</s>')도 함께 고려하면 좋습니다.\n",
    "    if tokenizer.eos_token: # tokenizer 변수가 사용 가능해야 합니다.\n",
    "         response_only = response_only.replace(tokenizer.eos_token, \"\")\n",
    "    \n",
    "    # eos_token_id=375가 '\\n'이라고 가정하고, 첫 줄바꿈 이전 텍스트만 사용하거나 불필요한 줄바꿈 제거\n",
    "    response_only = response_only.split('\\n')[0].strip()\n",
    "    \n",
    "    list_generated_responses.append(response_only)\n",
    "    # print(f\"  Q: {list_prompt_original[i]}\") # 디버깅용\n",
    "    # print(f\"  Extracted A: {response_only}\") # 디버깅용\n",
    "\n",
    "# 2. 참조 (정답) 텍스트 준비\n",
    "#    list_prompt_original의 각 질문에 대한 이상적인 답변을 작성합니다.\n",
    "#    이 목록은 모델의 답변과 비교될 \"정답지\" 역할을 합니다.\n",
    "list_references = [\n",
    "    \"일반적으로 불고기용 고기는 한우를 포함하여 다양한 종류를 사용할 수 있습니다. 요리의 종류나 개인의 취향에 따라 선택하시면 됩니다.\",\n",
    "    \"리처드 닉슨은 1953년부터 1961년까지 미국의 제36대 부통령으로 재임했습니다.\", # 43대가 아닌 36대입니다.\n",
    "    \"시카고 오헤어 국제공항은 미국 일리노이 주 시카고에 위치하고 있습니다.\",\n",
    "    \"오늘의 미세먼지 정보는 실시간으로 변동되므로, 기상청 웹사이트나 관련 앱을 통해 현재 계신 지역의 정보를 확인하시는 것이 가장 정확합니다.\"\n",
    "]\n",
    "\n",
    "# 생성된 응답과 참조 텍스트의 개수가 같은지 확인\n",
    "if len(list_generated_responses) != len(list_references):\n",
    "    print(f\"⚠️ 경고: 생성된 응답의 수({len(list_generated_responses)})와 참조 텍스트의 수({len(list_references)})가 다릅니다.\")\n",
    "    print(\"    ROUGE 스코어 계산을 위해 참조 텍스트 목록을 확인하고 조정해주세요.\")\n",
    "else:\n",
    "    # 3. 이전에 정의한 ROUGE 스코어 함수 호출\n",
    "    #    (calculate_and_print_rouge_scores 함수가 이전에 노트북에 정의되어 있어야 합니다)\n",
    "    print(\"\\n✨ ROUGE 스코어 함수를 호출하여 모델 성능 평가를 시작합니다...\")\n",
    "    \n",
    "    # calculate_and_print_rouge_scores 함수가 정의되어 있다고 가정하고 호출\n",
    "    # from rouge_score import rouge_scorer # 함수 내에 있으므로 여기서 다시 임포트할 필요는 없음\n",
    "    \n",
    "    average_rouge_scores = calculate_and_print_rouge_scores(\n",
    "        generated_texts=list_generated_responses,\n",
    "        reference_texts=list_references,\n",
    "        use_stemmer=False,  # 한국어의 경우 True로 설정하고 적절한 환경 구성 필요\n",
    "        print_individual_scores=True # 각 샘플별 점수 출력 여부\n",
    "    )\n",
    "\n",
    "    if average_rouge_scores:\n",
    "        print(\"\\n✅ ROUGE 점수 계산이 완료되었습니다!\")\n",
    "        # print(\"\\n반환된 평균 점수 딕셔너리 (F1 기준):\", average_rouge_scores) # 상세 결과 확인용\n",
    "    else:\n",
    "        print(\"❌ ROUGE 점수 계산에 실패했거나 결과가 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f27cd31",
   "metadata": {},
   "source": [
    "## ROUGE 스코어와 비교 평가\n",
    "각 질문들에 대해 제미나이가 예측한 문자열과 비교하여 ROUGE 평가를 진행하였다.\n",
    "\n",
    "제미나이가 생성해 낸 데이터들이 확인결과 팩트에 가까웠다.\n",
    "\n",
    "ex) 시카고오 국제공항의 위치 등등..\n",
    "\n",
    "n-gram을 1로 잡았을 때는 스코어가 어느 정도 나왔으나 \n",
    "\n",
    "n-gram이 2만 되어도 비슷한 텍스트가 전혀 겹치지 않을걸 확인할 수 있었다.\n",
    "\n",
    "즉, 단일 단어 수준에서 비슷하나 비슷한 문장은 생성해내지 못하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c5c35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e521a85",
   "metadata": {},
   "source": [
    "## RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0432bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# colossalai_ChatGPT_230319 라이브러리의 루트 경로를 sys.path에 추가\n",
    "# 현재 파일의 위치를 기준으로 상대 경로를 계산하거나, 절대 경로를 직접 지정할 수 있습니다.\n",
    "# 아래는 예시이며, 실제 경로에 맞게 수정해야 합니다.\n",
    "library_root_path = '/workspace/userdisk/KoChatGPT/colossalai_ChatGPT_230319'\n",
    "\n",
    "if library_root_path not in sys.path:\n",
    "    sys.path.insert(0, library_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "951eaff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from colossalai_ChatGPT_230319.chatgpt.dataset import RewardDataset\n",
    "from colossalai_ChatGPT_230319.chatgpt.models.base import RewardModel\n",
    "from colossalai_ChatGPT_230319.chatgpt.trainer import RewardModelTrainer\n",
    "from colossalai_ChatGPT_230319.chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dcb44107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ed761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17fd0cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "with open('./data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7396ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은?', 'chosen': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은 류승완의 사무실입니다.', 'rejected': '대구 영화사옥'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e33fa8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1266.74it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1134.51it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c73fee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a87388cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<03:56,  1.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<03:56,  1.05it/s, loss=0.335]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:54,  1.06it/s, loss=0.335]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:54,  1.06it/s, loss=0.248]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:54,  1.06it/s, loss=0.248]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:54,  1.06it/s, loss=0.817]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:53,  1.05it/s, loss=0.817]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:53,  1.05it/s, loss=0.794]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:52,  1.05it/s, loss=0.794]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:52,  1.05it/s, loss=0.409]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:52,  1.05it/s, loss=0.409]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:52,  1.05it/s, loss=0.227]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:52,  1.04it/s, loss=0.227]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:52,  1.04it/s, loss=0.32] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:07<03:52,  1.04it/s, loss=0.32]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:07<03:52,  1.04it/s, loss=0.447]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:08<03:52,  1.04it/s, loss=0.447]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:08<03:52,  1.04it/s, loss=0.351]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:09<03:52,  1.03it/s, loss=0.351]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:09<03:52,  1.03it/s, loss=0.399]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:10<03:51,  1.03it/s, loss=0.399]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:10<03:51,  1.03it/s, loss=0.315]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:11<03:51,  1.03it/s, loss=0.315]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:11<03:51,  1.03it/s, loss=0.383]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:12<03:52,  1.02it/s, loss=0.383]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:12<03:52,  1.02it/s, loss=0.0104]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:13<03:51,  1.02it/s, loss=0.0104]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:13<03:51,  1.02it/s, loss=3.1]   \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:14<03:51,  1.02it/s, loss=3.1]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:14<03:51,  1.02it/s, loss=0.269]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:15<03:51,  1.01it/s, loss=0.269]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:15<03:51,  1.01it/s, loss=1.72] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:16<03:51,  1.01it/s, loss=1.72]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:16<03:51,  1.01it/s, loss=0.789]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:17<03:52,  1.00s/it, loss=0.789]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:17<03:52,  1.00s/it, loss=0.451]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:18<03:51,  1.00s/it, loss=0.451]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:18<03:51,  1.00s/it, loss=0.633]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:19<03:51,  1.01s/it, loss=0.633]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:19<03:51,  1.01s/it, loss=0.601]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:20<03:51,  1.01s/it, loss=0.601]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:20<03:51,  1.01s/it, loss=0.686]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:21<03:51,  1.02s/it, loss=0.686]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:21<03:51,  1.02s/it, loss=0.598]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:22<03:51,  1.02s/it, loss=0.598]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:22<03:51,  1.02s/it, loss=1.08] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:23<03:51,  1.02s/it, loss=1.08]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:23<03:51,  1.02s/it, loss=0.545]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:24<03:51,  1.03s/it, loss=0.545]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:24<03:51,  1.03s/it, loss=0.367]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:25<03:51,  1.03s/it, loss=0.367]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:25<03:51,  1.03s/it, loss=0.811]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:26<03:50,  1.03s/it, loss=0.811]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:26<03:50,  1.03s/it, loss=0.622]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:27<03:50,  1.04s/it, loss=0.622]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:27<03:50,  1.04s/it, loss=0.602]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:28<03:50,  1.04s/it, loss=0.602]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:28<03:50,  1.04s/it, loss=0.442]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:29<03:50,  1.05s/it, loss=0.442]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:29<03:50,  1.05s/it, loss=0.509]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:31<03:50,  1.05s/it, loss=0.509]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:31<03:50,  1.05s/it, loss=0.404]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:32<03:50,  1.06s/it, loss=0.404]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:32<03:50,  1.06s/it, loss=0.826]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:33<03:49,  1.06s/it, loss=0.826]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:33<03:49,  1.06s/it, loss=0.441]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:34<03:49,  1.06s/it, loss=0.441]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:34<03:49,  1.06s/it, loss=0.536]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:35<03:49,  1.07s/it, loss=0.536]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:35<03:49,  1.07s/it, loss=1.08] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:36<03:48,  1.07s/it, loss=1.08]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:36<03:48,  1.07s/it, loss=1.21]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:37<03:48,  1.07s/it, loss=1.21]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:37<03:48,  1.07s/it, loss=0.829]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:38<03:47,  1.07s/it, loss=0.829]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:38<03:47,  1.07s/it, loss=0.608]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:39<03:46,  1.07s/it, loss=0.608]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:39<03:46,  1.07s/it, loss=0.856]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:40<03:45,  1.08s/it, loss=0.856]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:40<03:45,  1.08s/it, loss=0.51] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:41<03:44,  1.07s/it, loss=0.51]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:41<03:44,  1.07s/it, loss=0.499]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:42<03:43,  1.07s/it, loss=0.499]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:42<03:43,  1.07s/it, loss=0.621]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:43<03:42,  1.07s/it, loss=0.621]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:43<03:42,  1.07s/it, loss=0.811]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:44<03:40,  1.07s/it, loss=0.811]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:44<03:40,  1.07s/it, loss=1.25] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:46<03:39,  1.07s/it, loss=1.25]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:46<03:39,  1.07s/it, loss=0.584]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:47<03:37,  1.07s/it, loss=0.584]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:47<03:37,  1.07s/it, loss=0.923]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:48<03:35,  1.06s/it, loss=0.923]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:48<03:35,  1.06s/it, loss=0.732]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:49<03:33,  1.06s/it, loss=0.732]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:49<03:33,  1.06s/it, loss=0.666]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:50<03:31,  1.05s/it, loss=0.666]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:50<03:31,  1.05s/it, loss=0.659]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:51<03:29,  1.05s/it, loss=0.659]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:51<03:29,  1.05s/it, loss=0.865]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:52<03:27,  1.04s/it, loss=0.865]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:52<03:27,  1.04s/it, loss=0.604]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:53<03:25,  1.04s/it, loss=0.604]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:53<03:25,  1.04s/it, loss=0.623]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:54<03:23,  1.03s/it, loss=0.623]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:54<03:23,  1.03s/it, loss=0.65] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:55<03:22,  1.03s/it, loss=0.65]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:55<03:22,  1.03s/it, loss=0.525]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:56<03:20,  1.03s/it, loss=0.525]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:56<03:20,  1.03s/it, loss=0.61] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:57<03:19,  1.03s/it, loss=0.61]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:57<03:19,  1.03s/it, loss=0.66]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:58<03:17,  1.03s/it, loss=0.66]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:58<03:17,  1.03s/it, loss=0.555]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:59<03:16,  1.02s/it, loss=0.555]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:59<03:16,  1.02s/it, loss=0.53] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [01:00<03:15,  1.02s/it, loss=0.53]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [01:00<03:15,  1.02s/it, loss=0.522]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [01:01<03:13,  1.02s/it, loss=0.522]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [01:01<03:13,  1.02s/it, loss=0.444]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [01:02<03:12,  1.02s/it, loss=0.444]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [01:02<03:12,  1.02s/it, loss=0.799]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [01:03<03:10,  1.02s/it, loss=0.799]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [01:03<03:10,  1.02s/it, loss=0.342]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [01:04<03:09,  1.01s/it, loss=0.342]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [01:04<03:09,  1.01s/it, loss=0.756]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [01:05<03:08,  1.01s/it, loss=0.756]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [01:05<03:08,  1.01s/it, loss=0.352]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [01:06<03:06,  1.01s/it, loss=0.352]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [01:06<03:06,  1.01s/it, loss=0.667]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [01:07<03:05,  1.01s/it, loss=0.667]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [01:07<03:05,  1.01s/it, loss=0.202]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [01:08<03:04,  1.01s/it, loss=0.202]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [01:08<03:04,  1.01s/it, loss=0.875]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [01:09<03:03,  1.01s/it, loss=0.875]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [01:09<03:03,  1.01s/it, loss=0.337]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:10<03:01,  1.01s/it, loss=0.337]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:10<03:01,  1.01s/it, loss=0.821]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:11<03:00,  1.00s/it, loss=0.821]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:11<03:00,  1.00s/it, loss=0.76] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:12<02:59,  1.00s/it, loss=0.76]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:12<02:59,  1.00s/it, loss=0.506]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:13<02:58,  1.00s/it, loss=0.506]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:13<02:58,  1.00s/it, loss=0.83] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:14<02:56,  1.00it/s, loss=0.83]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:14<02:56,  1.00it/s, loss=0.821]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:15<02:56,  1.00s/it, loss=0.821]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:15<02:56,  1.00s/it, loss=0.6]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:16<02:54,  1.00it/s, loss=0.6]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:16<02:54,  1.00it/s, loss=0.616]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:17<02:53,  1.00it/s, loss=0.616]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:17<02:53,  1.00it/s, loss=0.739]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:18<02:52,  1.00it/s, loss=0.739]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:18<02:52,  1.00it/s, loss=0.414]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:19<02:51,  1.00it/s, loss=0.414]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:19<02:51,  1.00it/s, loss=0.568]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:20<02:50,  1.00it/s, loss=0.568]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:20<02:50,  1.00it/s, loss=0.543]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:21<02:49,  1.00it/s, loss=0.543]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:21<02:49,  1.00it/s, loss=0.685]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:22<02:48,  1.01it/s, loss=0.685]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:22<02:48,  1.01it/s, loss=0.356]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:23<02:47,  1.00it/s, loss=0.356]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:23<02:47,  1.00it/s, loss=0.79] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:24<02:46,  1.00it/s, loss=0.79]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:24<02:46,  1.00it/s, loss=0.554]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:25<02:45,  1.00it/s, loss=0.554]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:25<02:45,  1.00it/s, loss=0.671]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:26<02:44,  1.01it/s, loss=0.671]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:26<02:44,  1.01it/s, loss=0.737]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:27<02:43,  1.00it/s, loss=0.737]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:27<02:43,  1.00it/s, loss=0.584]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:28<02:42,  1.00it/s, loss=0.584]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:28<02:42,  1.00it/s, loss=0.52] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:29<02:41,  1.01it/s, loss=0.52]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:29<02:41,  1.01it/s, loss=0.928]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:30<02:40,  1.00it/s, loss=0.928]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:30<02:40,  1.00it/s, loss=0.667]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:31<02:39,  1.00it/s, loss=0.667]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:31<02:39,  1.00it/s, loss=0.805]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:32<02:38,  1.00it/s, loss=0.805]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:32<02:38,  1.00it/s, loss=0.614]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:33<02:37,  1.00it/s, loss=0.614]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:33<02:37,  1.00it/s, loss=0.566]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:34<02:36,  1.00it/s, loss=0.566]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:34<02:36,  1.00it/s, loss=0.954]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:35<02:35,  1.00it/s, loss=0.954]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:35<02:35,  1.00it/s, loss=0.691]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:36<02:35,  1.00s/it, loss=0.691]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:36<02:35,  1.00s/it, loss=0.507]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:37<02:34,  1.00s/it, loss=0.507]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:37<02:34,  1.00s/it, loss=0.658]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:38<02:33,  1.00s/it, loss=0.658]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:38<02:33,  1.00s/it, loss=0.702]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:39<02:32,  1.01s/it, loss=0.702]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:39<02:32,  1.01s/it, loss=0.683]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:40<02:31,  1.01s/it, loss=0.683]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:40<02:31,  1.01s/it, loss=0.528]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:41<02:30,  1.01s/it, loss=0.528]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:41<02:30,  1.01s/it, loss=0.584]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:42<02:29,  1.00s/it, loss=0.584]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:42<02:29,  1.00s/it, loss=0.665]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:43<02:28,  1.00s/it, loss=0.665]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:43<02:28,  1.00s/it, loss=0.643]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:44<02:27,  1.01s/it, loss=0.643]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:44<02:27,  1.01s/it, loss=0.691]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:45<02:27,  1.01s/it, loss=0.691]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:45<02:27,  1.01s/it, loss=0.807]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:46<02:26,  1.01s/it, loss=0.807]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:46<02:26,  1.01s/it, loss=0.73] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:47<02:25,  1.01s/it, loss=0.73]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:47<02:25,  1.01s/it, loss=0.645]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:48<02:24,  1.01s/it, loss=0.645]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:48<02:24,  1.01s/it, loss=0.832]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:49<02:23,  1.01s/it, loss=0.832]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:49<02:23,  1.01s/it, loss=0.566]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:50<02:22,  1.01s/it, loss=0.566]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:50<02:22,  1.01s/it, loss=0.659]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:51<02:21,  1.01s/it, loss=0.659]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:51<02:21,  1.01s/it, loss=0.813]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:52<02:20,  1.01s/it, loss=0.813]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:52<02:20,  1.01s/it, loss=0.584]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:53<02:20,  1.01s/it, loss=0.584]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:53<02:20,  1.01s/it, loss=0.675]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:54<02:18,  1.01s/it, loss=0.675]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:54<02:18,  1.01s/it, loss=0.649]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:55<02:17,  1.01s/it, loss=0.649]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:55<02:17,  1.01s/it, loss=0.819]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:56<02:17,  1.02s/it, loss=0.819]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:56<02:17,  1.02s/it, loss=0.646]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:57<02:16,  1.02s/it, loss=0.646]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:57<02:16,  1.02s/it, loss=0.489]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:58<02:15,  1.02s/it, loss=0.489]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:58<02:15,  1.02s/it, loss=0.458]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:59<02:14,  1.02s/it, loss=0.458]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:59<02:14,  1.02s/it, loss=0.417]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [02:00<02:13,  1.02s/it, loss=0.417]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [02:00<02:13,  1.02s/it, loss=0.53] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [02:01<02:12,  1.02s/it, loss=0.53]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [02:01<02:12,  1.02s/it, loss=1.37]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [02:02<02:11,  1.02s/it, loss=1.37]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [02:02<02:11,  1.02s/it, loss=0.795]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [02:03<02:11,  1.03s/it, loss=0.795]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [02:03<02:11,  1.03s/it, loss=0.601]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [02:04<02:10,  1.03s/it, loss=0.601]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [02:04<02:10,  1.03s/it, loss=0.488]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [02:05<02:09,  1.02s/it, loss=0.488]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [02:05<02:09,  1.02s/it, loss=0.599]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [02:06<02:08,  1.02s/it, loss=0.599]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [02:06<02:08,  1.02s/it, loss=0.618]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [02:07<02:07,  1.02s/it, loss=0.618]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [02:07<02:07,  1.02s/it, loss=0.721]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [02:08<02:06,  1.02s/it, loss=0.721]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [02:08<02:06,  1.02s/it, loss=0.663]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [02:09<02:04,  1.02s/it, loss=0.663]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [02:10<02:04,  1.02s/it, loss=0.753]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [02:11<02:03,  1.02s/it, loss=0.753]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [02:11<02:03,  1.02s/it, loss=0.636]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [02:12<02:02,  1.02s/it, loss=0.636]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [02:12<02:02,  1.02s/it, loss=0.718]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [02:13<02:01,  1.02s/it, loss=0.718]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [02:13<02:01,  1.02s/it, loss=0.933]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [02:14<02:00,  1.02s/it, loss=0.933]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [02:14<02:00,  1.02s/it, loss=0.609]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [02:15<01:59,  1.02s/it, loss=0.609]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [02:15<01:59,  1.02s/it, loss=0.799]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [02:16<01:58,  1.02s/it, loss=0.799]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [02:16<01:58,  1.02s/it, loss=0.73] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [02:17<01:57,  1.02s/it, loss=0.73]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [02:17<01:57,  1.02s/it, loss=0.685]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [02:18<01:56,  1.02s/it, loss=0.685]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [02:18<01:56,  1.02s/it, loss=0.644]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:19<01:55,  1.02s/it, loss=0.644]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:19<01:55,  1.02s/it, loss=0.566]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:20<01:54,  1.02s/it, loss=0.566]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:20<01:54,  1.02s/it, loss=0.587]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:21<01:53,  1.02s/it, loss=0.587]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:21<01:53,  1.02s/it, loss=0.533]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:22<01:52,  1.02s/it, loss=0.533]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:22<01:52,  1.02s/it, loss=0.503]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:23<01:51,  1.02s/it, loss=0.503]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:23<01:51,  1.02s/it, loss=0.632]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:24<01:50,  1.02s/it, loss=0.632]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:24<01:50,  1.02s/it, loss=0.836]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:25<01:49,  1.02s/it, loss=0.836]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:25<01:49,  1.02s/it, loss=0.62] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:26<01:48,  1.02s/it, loss=0.62]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:26<01:48,  1.02s/it, loss=0.482]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:27<01:47,  1.02s/it, loss=0.482]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:27<01:47,  1.02s/it, loss=0.646]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:28<01:45,  1.02s/it, loss=0.646]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:28<01:45,  1.02s/it, loss=0.571]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:29<01:44,  1.02s/it, loss=0.571]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:29<01:44,  1.02s/it, loss=0.387]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:30<01:43,  1.02s/it, loss=0.387]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:30<01:43,  1.02s/it, loss=0.675]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:31<01:42,  1.02s/it, loss=0.675]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:31<01:42,  1.02s/it, loss=0.607]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:32<01:41,  1.02s/it, loss=0.607]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:32<01:41,  1.02s/it, loss=0.504]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:33<01:40,  1.01s/it, loss=0.504]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:33<01:40,  1.01s/it, loss=0.348]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:34<01:39,  1.01s/it, loss=0.348]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:34<01:39,  1.01s/it, loss=0.618]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:35<01:38,  1.01s/it, loss=0.618]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:35<01:38,  1.01s/it, loss=0.788]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:36<01:37,  1.01s/it, loss=0.788]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:36<01:37,  1.01s/it, loss=0.848]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:37<01:36,  1.01s/it, loss=0.848]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:37<01:36,  1.01s/it, loss=1.05] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:38<01:35,  1.01s/it, loss=1.05]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:38<01:35,  1.01s/it, loss=0.419]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:39<01:34,  1.01s/it, loss=0.419]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:39<01:34,  1.01s/it, loss=0.607]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:40<01:33,  1.01s/it, loss=0.607]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:40<01:33,  1.01s/it, loss=0.71] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:41<01:31,  1.01s/it, loss=0.71]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:41<01:31,  1.01s/it, loss=0.792]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:42<01:30,  1.01s/it, loss=0.792]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:42<01:30,  1.01s/it, loss=0.36] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:43<01:29,  1.01s/it, loss=0.36]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:43<01:29,  1.01s/it, loss=0.694]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:44<01:28,  1.01s/it, loss=0.694]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:44<01:28,  1.01s/it, loss=0.488]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:45<01:28,  1.01s/it, loss=0.488]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:45<01:28,  1.01s/it, loss=0.373]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:46<01:26,  1.01s/it, loss=0.373]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:46<01:26,  1.01s/it, loss=0.493]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:47<01:25,  1.01s/it, loss=0.493]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:47<01:25,  1.01s/it, loss=1.07] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:48<01:24,  1.01s/it, loss=1.07]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:48<01:24,  1.01s/it, loss=0.351]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:49<01:23,  1.01s/it, loss=0.351]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:49<01:23,  1.01s/it, loss=0.378]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:50<01:22,  1.01s/it, loss=0.378]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:50<01:22,  1.01s/it, loss=0.576]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:51<01:21,  1.01s/it, loss=0.576]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:51<01:21,  1.01s/it, loss=0.414]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:52<01:20,  1.01s/it, loss=0.414]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:52<01:20,  1.01s/it, loss=1.02] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:53<01:19,  1.01s/it, loss=1.02]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:53<01:19,  1.01s/it, loss=0.648]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:54<01:18,  1.01s/it, loss=0.648]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:54<01:18,  1.01s/it, loss=0.811]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:55<01:17,  1.01s/it, loss=0.811]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:55<01:17,  1.01s/it, loss=0.372]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:56<01:16,  1.01s/it, loss=0.372]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:56<01:16,  1.01s/it, loss=0.431]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:57<01:15,  1.01s/it, loss=0.431]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:57<01:15,  1.01s/it, loss=0.566]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:58<01:14,  1.01s/it, loss=0.566]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:58<01:14,  1.01s/it, loss=0.411]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:59<01:13,  1.01s/it, loss=0.411]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:59<01:13,  1.01s/it, loss=0.365]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [03:00<01:12,  1.01s/it, loss=0.365]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [03:00<01:12,  1.01s/it, loss=0.348]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [03:01<01:11,  1.01s/it, loss=0.348]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [03:01<01:11,  1.01s/it, loss=0.528]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [03:02<01:10,  1.01s/it, loss=0.528]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [03:02<01:10,  1.01s/it, loss=0.741]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [03:03<01:09,  1.01s/it, loss=0.741]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [03:03<01:09,  1.01s/it, loss=0.327]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [03:04<01:08,  1.01s/it, loss=0.327]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [03:04<01:08,  1.01s/it, loss=1.03] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [03:05<01:07,  1.01s/it, loss=1.03]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [03:05<01:07,  1.01s/it, loss=1.1] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [03:06<01:06,  1.01s/it, loss=1.1]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [03:06<01:06,  1.01s/it, loss=0.782]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [03:07<01:05,  1.01s/it, loss=0.782]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [03:07<01:05,  1.01s/it, loss=0.575]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [03:08<01:04,  1.01s/it, loss=0.575]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [03:08<01:04,  1.01s/it, loss=0.382]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [03:09<01:03,  1.01s/it, loss=0.382]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [03:09<01:03,  1.01s/it, loss=0.342]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [03:10<01:02,  1.01s/it, loss=0.342]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [03:10<01:02,  1.01s/it, loss=0.5]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [03:11<01:01,  1.01s/it, loss=0.5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [03:11<01:01,  1.01s/it, loss=0.807]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [03:12<01:00,  1.01s/it, loss=0.807]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [03:12<01:00,  1.01s/it, loss=0.443]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [03:13<00:59,  1.01s/it, loss=0.443]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [03:13<00:59,  1.01s/it, loss=0.234]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [03:14<00:58,  1.01s/it, loss=0.234]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [03:14<00:58,  1.01s/it, loss=0.484]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [03:15<00:57,  1.01s/it, loss=0.484]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [03:15<00:57,  1.01s/it, loss=0.974]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [03:16<00:56,  1.01s/it, loss=0.974]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [03:16<00:56,  1.01s/it, loss=0.729]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [03:17<00:55,  1.01s/it, loss=0.729]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [03:17<00:55,  1.01s/it, loss=0.393]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [03:18<00:54,  1.00s/it, loss=0.393]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [03:18<00:54,  1.00s/it, loss=0.993]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [03:19<00:53,  1.00s/it, loss=0.993]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [03:19<00:53,  1.00s/it, loss=0.562]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [03:20<00:52,  1.00s/it, loss=0.562]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [03:20<00:52,  1.00s/it, loss=0.591]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [03:21<00:51,  1.00s/it, loss=0.591]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [03:21<00:51,  1.00s/it, loss=0.405]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [03:22<00:50,  1.01s/it, loss=0.405]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [03:22<00:50,  1.01s/it, loss=0.419]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [03:23<00:49,  1.01s/it, loss=0.419]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [03:23<00:49,  1.01s/it, loss=0.427]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [03:24<00:48,  1.01s/it, loss=0.427]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [03:24<00:48,  1.01s/it, loss=0.781]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [03:25<00:47,  1.01s/it, loss=0.781]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [03:25<00:47,  1.01s/it, loss=0.608]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [03:26<00:46,  1.01s/it, loss=0.608]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [03:26<00:46,  1.01s/it, loss=0.499]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:27<00:45,  1.01s/it, loss=0.499]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:27<00:45,  1.01s/it, loss=0.55] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:28<00:44,  1.01s/it, loss=0.55]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:28<00:44,  1.01s/it, loss=0.8] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:29<00:43,  1.01s/it, loss=0.8]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:29<00:43,  1.01s/it, loss=0.495]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:30<00:42,  1.01s/it, loss=0.495]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:30<00:42,  1.01s/it, loss=0.739]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:31<00:41,  1.01s/it, loss=0.739]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:31<00:41,  1.01s/it, loss=0.695]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:32<00:40,  1.01s/it, loss=0.695]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:32<00:40,  1.01s/it, loss=0.73] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:33<00:39,  1.01s/it, loss=0.73]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:33<00:39,  1.01s/it, loss=0.379]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:34<00:38,  1.01s/it, loss=0.379]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:34<00:38,  1.01s/it, loss=0.516]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:35<00:37,  1.01s/it, loss=0.516]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:35<00:37,  1.01s/it, loss=0.386]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:36<00:36,  1.01s/it, loss=0.386]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:36<00:36,  1.01s/it, loss=1.03] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:37<00:35,  1.01s/it, loss=1.03]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:37<00:35,  1.01s/it, loss=0.495]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:38<00:34,  1.01s/it, loss=0.495]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:38<00:34,  1.01s/it, loss=0.815]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:39<00:33,  1.01s/it, loss=0.815]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:39<00:33,  1.01s/it, loss=0.886]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:40<00:32,  1.01s/it, loss=0.886]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:40<00:32,  1.01s/it, loss=0.578]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:41<00:31,  1.01s/it, loss=0.578]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:41<00:31,  1.01s/it, loss=0.365]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:42<00:30,  1.01s/it, loss=0.365]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:42<00:30,  1.01s/it, loss=0.66] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:43<00:29,  1.01s/it, loss=0.66]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:44<00:29,  1.01s/it, loss=0.709]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:44<00:28,  1.01s/it, loss=0.709]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:45<00:28,  1.01s/it, loss=0.294]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:45<00:27,  1.01s/it, loss=0.294]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:46<00:27,  1.01s/it, loss=0.723]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:47<00:26,  1.01s/it, loss=0.723]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:47<00:26,  1.01s/it, loss=0.829]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:48<00:25,  1.01s/it, loss=0.829]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:48<00:25,  1.01s/it, loss=0.612]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:49<00:24,  1.01s/it, loss=0.612]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:49<00:24,  1.01s/it, loss=0.629]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:50<00:23,  1.01s/it, loss=0.629]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:50<00:23,  1.01s/it, loss=0.808]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:51<00:22,  1.01s/it, loss=0.808]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:51<00:22,  1.01s/it, loss=0.503]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:52<00:21,  1.01s/it, loss=0.503]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:52<00:21,  1.01s/it, loss=0.707]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:53<00:20,  1.01s/it, loss=0.707]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:53<00:20,  1.01s/it, loss=0.752]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:54<00:19,  1.01s/it, loss=0.752]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:54<00:19,  1.01s/it, loss=0.455]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:55<00:18,  1.01s/it, loss=0.455]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:55<00:18,  1.01s/it, loss=0.839]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:56<00:17,  1.01s/it, loss=0.839]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:56<00:17,  1.01s/it, loss=0.491]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:57<00:16,  1.01s/it, loss=0.491]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:57<00:16,  1.01s/it, loss=0.581]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:58<00:15,  1.01s/it, loss=0.581]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:58<00:15,  1.01s/it, loss=0.864]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:59<00:14,  1.01s/it, loss=0.864]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:59<00:14,  1.01s/it, loss=0.634]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [04:00<00:13,  1.01s/it, loss=0.634]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [04:00<00:13,  1.01s/it, loss=0.539]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [04:01<00:12,  1.01s/it, loss=0.539]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [04:01<00:12,  1.01s/it, loss=0.74] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [04:02<00:11,  1.01s/it, loss=0.74]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [04:02<00:11,  1.01s/it, loss=0.669]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [04:03<00:10,  1.01s/it, loss=0.669]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [04:03<00:10,  1.01s/it, loss=0.575]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [04:04<00:09,  1.01s/it, loss=0.575]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [04:04<00:09,  1.01s/it, loss=0.572]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [04:05<00:08,  1.01s/it, loss=0.572]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [04:05<00:08,  1.01s/it, loss=0.741]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [04:06<00:07,  1.01s/it, loss=0.741]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [04:06<00:07,  1.01s/it, loss=0.576]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [04:07<00:06,  1.01s/it, loss=0.576]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [04:07<00:06,  1.01s/it, loss=0.634]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [04:08<00:05,  1.01s/it, loss=0.634]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [04:08<00:05,  1.01s/it, loss=0.77] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [04:09<00:04,  1.01s/it, loss=0.77]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [04:09<00:04,  1.01s/it, loss=0.782]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [04:10<00:03,  1.01s/it, loss=0.782]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [04:10<00:03,  1.01s/it, loss=0.431]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [04:11<00:02,  1.01s/it, loss=0.431]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [04:11<00:02,  1.01s/it, loss=1.21] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [04:12<00:01,  1.01s/it, loss=1.21]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [04:12<00:01,  1.01s/it, loss=0.651]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [04:13<00:00,  1.01s/it, loss=0.651]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [04:13<00:00,  1.01s/it, loss=0.551]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Train epoch: 100%|██████████| 1/1 [04:30<00:00, 270.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [04:30<00:00,  1.08s/it, loss=0.616, dist_mean=0.27]\u001b[A\u001b[A\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [04:30<00:00, 270.28s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('/KoChatGPT/output_2_RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "919f08b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 대한민국에서 가장 높은 산의 이름과 높이는 무엇인가요?\n",
      "reward score: -1.1\n"
     ]
    }
   ],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '대한민국에서 가장 높은 산의 이름과 높이는 무엇인가요?'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8ab23a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.\n",
      "reward score: -0.7\n"
     ]
    }
   ],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f251bfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\n",
      "reward score: -0.5\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e845e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\n",
      "reward score: -0.4\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "15c7067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "43669689",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='/KoChatGPT/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='/KoChatGPT/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0425844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e7217485",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e8af8e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "84e40f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4bdfcef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Episode [1/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [1/10]:  33%|███▎      | 1/3 [00:06<00:13,  6.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:13<00:06,  6.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.000338]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.61it/s, actor_loss=0, critic_loss=0.000338]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.61it/s, actor_loss=0, critic_loss=0.164]   \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.68it/s, actor_loss=0, critic_loss=0.164]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.68it/s, actor_loss=0, critic_loss=0.0113]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.69it/s, actor_loss=0, critic_loss=0.0113]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [1/10]: 100%|██████████| 3/3 [00:22<00:00,  7.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [2/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [2/10]:  33%|███▎      | 1/3 [00:07<00:14,  7.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:14<00:07,  7.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.206, critic_loss=0.0406]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.59it/s, actor_loss=0.206, critic_loss=0.0406]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.59it/s, actor_loss=0.295, critic_loss=0.0926]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.59it/s, actor_loss=0.295, critic_loss=0.0926]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.59it/s, actor_loss=0.198, critic_loss=0.0672]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.59it/s, actor_loss=0.198, critic_loss=0.0672]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [2/10]: 100%|██████████| 3/3 [00:23<00:00,  7.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [3/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [3/10]:  33%|███▎      | 1/3 [00:07<00:14,  7.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:14<00:07,  7.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.13, critic_loss=0.019]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s, actor_loss=0.13, critic_loss=0.019]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.65it/s, actor_loss=0.117, critic_loss=0.00108]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.67it/s, actor_loss=0.117, critic_loss=0.00108]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.67it/s, actor_loss=0.13, critic_loss=0.022]   \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.67it/s, actor_loss=0.13, critic_loss=0.022]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [3/10]: 100%|██████████| 3/3 [00:23<00:00,  7.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [4/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [4/10]:  33%|███▎      | 1/3 [00:07<00:14,  7.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:14<00:07,  7.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.19, critic_loss=0.0413]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.64it/s, actor_loss=-.19, critic_loss=0.0413]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.64it/s, actor_loss=-.194, critic_loss=0.041]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.66it/s, actor_loss=-.194, critic_loss=0.041]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.66it/s, actor_loss=-.208, critic_loss=0.0238]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.67it/s, actor_loss=-.208, critic_loss=0.0238]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [4/10]: 100%|██████████| 3/3 [00:22<00:00,  7.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [5/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [5/10]:  33%|███▎      | 1/3 [00:06<00:13,  6.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:13<00:06,  6.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0578, critic_loss=0.00362]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s, actor_loss=-.0578, critic_loss=0.00362]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.65it/s, actor_loss=-.0507, critic_loss=0.00304]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.65it/s, actor_loss=-.0507, critic_loss=0.00304]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.65it/s, actor_loss=-.0483, critic_loss=0.0198] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.65it/s, actor_loss=-.0483, critic_loss=0.0198]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [5/10]: 100%|██████████| 3/3 [00:22<00:00,  7.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [6/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [6/10]:  33%|███▎      | 1/3 [00:07<00:14,  7.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:14<00:07,  7.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.16, critic_loss=0.0234]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.63it/s, actor_loss=0.16, critic_loss=0.0234]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.63it/s, actor_loss=0.172, critic_loss=0.0279]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.63it/s, actor_loss=0.172, critic_loss=0.0279]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.63it/s, actor_loss=0.162, critic_loss=0.0124]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.64it/s, actor_loss=0.162, critic_loss=0.0124]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [6/10]: 100%|██████████| 3/3 [00:23<00:00,  7.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [7/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [7/10]:  33%|███▎      | 1/3 [00:07<00:14,  7.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:14<00:07,  7.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0134, critic_loss=0.00229]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.65it/s, actor_loss=0.0134, critic_loss=0.00229]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.65it/s, actor_loss=0.00472, critic_loss=0.00411]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.66it/s, actor_loss=0.00472, critic_loss=0.00411]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.66it/s, actor_loss=0.0122, critic_loss=0.0125]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, actor_loss=0.0122, critic_loss=0.0125]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [7/10]: 100%|██████████| 3/3 [00:23<00:00,  7.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [8/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [8/10]:  33%|███▎      | 1/3 [00:06<00:13,  6.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [8/10]:  67%|██████▋   | 2/3 [00:13<00:06,  6.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.108, critic_loss=0.00957]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.64it/s, actor_loss=-.108, critic_loss=0.00957]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.64it/s, actor_loss=-.127, critic_loss=0.0132] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.66it/s, actor_loss=-.127, critic_loss=0.0132]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.66it/s, actor_loss=-.116, critic_loss=0.00929]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.66it/s, actor_loss=-.116, critic_loss=0.00929]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [8/10]: 100%|██████████| 3/3 [00:22<00:00,  7.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [9/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [9/10]:  33%|███▎      | 1/3 [00:07<00:14,  7.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0279, critic_loss=0.00136]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.66it/s, actor_loss=-.0279, critic_loss=0.00136]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.66it/s, actor_loss=-.0236, critic_loss=0.00178]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.67it/s, actor_loss=-.0236, critic_loss=0.00178]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.67it/s, actor_loss=-.017, critic_loss=0.0102]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.67it/s, actor_loss=-.017, critic_loss=0.0102]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [9/10]: 100%|██████████| 3/3 [00:21<00:00,  7.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [10/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [10/10]:  33%|███▎      | 1/3 [00:06<00:13,  6.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:13<00:07,  7.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0972, critic_loss=0.0129]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.66it/s, actor_loss=0.0972, critic_loss=0.0129]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.66it/s, actor_loss=0.0952, critic_loss=0.00798]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.67it/s, actor_loss=0.0952, critic_loss=0.00798]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.67it/s, actor_loss=0.0928, critic_loss=0.00192]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.67it/s, actor_loss=0.0928, critic_loss=0.00192]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [10/10]: 100%|██████████| 3/3 [00:20<00:00,  7.00s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('/KoChatGPT/output_3_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5bbc20a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'죄송합니다, 제가 인공지능 챗봇이므로 이에 대한 정보를 알지 못합니다. 하지만 \"불고기\"가 어느 지역인지 알려주시면 더 자세한 정보를 알려드리도록 하겠습니다. 감사합니다. 神文字「宗書」입니다. 臣衆書 『東]입니다. \"불고기용 고기 한우에 관한 한우(韓牛)라는 말이 있습니다. \"불고기용 고기 한우는 우리가 알고 있는 한우(韓牛)라는 뜻으로, 많은 지역에서 자라는 쇠고기의 한우를 일컫는 말입니다. 神文字 「自」(또는)입니다. 高書』입니다. 高 書 『神文字「自」입니다. 神文字「自」께서 \"불고기용 한우는 우리가 알고 있는 한우를 말하는 말입니다. 神書「自」의 뜻이 \"불고기용 고기 한우의 한우(韓牛)라고 하였습니다.\", 'token': 174} 信文者「自」 \"神書 《不孤高牛\" 孝隆之法( 孝法制無助李\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 55대 부통령직을 수행했습니다. 1952년 12월 10일, 리처드 닉슨은 32대 부통령직을 수행했습니다. 1952년 1월 11일: 그는 부통령직을 수행하지 않았습니다. 필리프 닉슨은 50대 부통령직을 수행했습니다. 필리프 닉슨은 50대 부통령직을 수행했습니다. 필리프 닉슨은 35대 부통령직을 수행했습니다. 필리프 닉슨은 50대 부통령직을 수행했습니다. 필리프 닉슨은 35대 부통령직을 수행했습니다. 필리프 닉슨은 43대 부통령직을 수행했습니다. 필리프 닉슨은 53대 부통령직을 수행했습니다. 필리프 닉슨은 39대 부통령직을 수행했습니다. 필리프 닉슨은 41대 부통령직을 수행했습니다. 필리프 닉슨은 30대 부통령직을 수행했습니다. 필리프 닉슨은 50대 부통령직을 수행했습니다. 필리프 닉슨은 30대 부통령직을 수행했습니다. 필리프 닉슨은 30대 부통령직을 수행했습니다. 필리프 닉슨은 31대 부통령직을 수행했습니다. 필리프 닉슨은 30대 부통령직을 수행했습니다. 필리프 닉슨은 40대 부통령직을 수행\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오헤어 국제공항은 미국 미네소타주에 위치해 있습니다.英審國際 국제공항이라고도 불립니다.英審國際 국제공항은 미국 미네소타주 시카고의 국제공항으로, 현재는 미국 중서부에 위치해 있습니다.英國際 국제공항은 미국 미네소타주의 세인트루이스 주에 위치해 있습니다.英國際 국제공항은 1869년 3월 15일에 설립되었습니다.英国際 국제공항은 미국 미네소타주 알론시아나에게 위치해 있으며, 현재는 국제공항으로 사용되고 있습니다.英國際 국제공항은 현재까지도 미국 중서부에서 매우 인기있는 도시 중 하나입니다.英国際 국제공항은 미국 중서부에서 매우 유명한 공항 중 하나로 여겨지며, 다른 지역의 공항이 될 수 있다는 것을 보여주고 있습니다.英国際 국제공항은 미국 중서부에 위치한 나라입니다.英國際 국제공항은 1911년 3월 15일에 미국에 처음으로 개항하였으며, 현재는 일본 정부 관할 하에 있는 것으로 알려져 있습니다.英國際 국제공항은 1906년에 미국의 도시도시 상에서 발견되었으며, 당시 세계 최연소로 여겨지고 있습니다. 英國際 국제공항은 주로 미국에 위치해 있습니다.英國際 국제공항은 1920년대에 처음 존재한 국제공항으로, 현재까지도 미국에서 여러\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'미세먼지 정보를 확인하셔야 어떤 미세먼지 정보를 말씀하시는지 알려주시면 감사하겠습니다. 현재 진행중인 미세먼지 제거와 같은 활동을 적극적으로 활용하시기 바랍니다. 현재는 미세먼지 측정 시스템이 설치되어 있습니다. 자세한 정보가 필요하시면 전문가와 상담하시면 도움을 받을 수 있습니다. 상세한 정보를 얻을 수 있도록 최선의 노력을 하세요.箚直士와 결탁한 행위는 불건전한 행동 중 하나입니다.伺輩의 영향을 받아 미세먼지를 제거하고 다시 한번 노력하시겠다는 약속을 하셨습니다.前 戊辰國(황중)後文를 맡은 이력은 있으니, 참고 자료로 남겨두시길 바라며, 감사합니다.前者的自由神論を 맡고 계시던 戊辰國,後的自由臣無安歸去, 黑田無安國의 지혜를 참고하시면 됩니다.前前者前者, 聖中仁聖衆의 가르침을 받들어주시길 바랍니다. 後業的\n"
     ]
    }
   ],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b1d75",
   "metadata": {},
   "source": [
    "## PPO - 정성평가\n",
    "전체적으로 대답의 길이가 길어졌고 쫌 더 우리가 사용하는 프론티어 모델과 비슷한 느낌이 나긴한다.\n",
    "\n",
    "하지만 단순하게 문장의 길이만 길어졌을 뿐 정말 말도 안되는 문장들을 생성해내고 그마저도 반복적이라는 느낌이 든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3e67a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ROUGE Score Results ---\n",
      "⚠️ 생성된 답변이 없습니다. `generation` 함수를 호출하여 `generated_answers` 리스트를 채워주세요.\n",
      "   또는, 수동으로 `generated_answers` 리스트에 PPO 모델의 답변을 넣어주세요.\n",
      "\n",
      "Prompt Question: 불고기용 고기 한우에요?\n",
      "Generated: 죄송합니다, 제가 인공지능 챗봇이므로 이에 대한 정보를 알지 못합니다. 하지만 불고기가 어느 지역인지 알려주시면 더 자세한 정보를 알려드리도록 하겠습니다. 감사합니다. 神文字「宗書」입니다. 臣衆書 『東]입니다. 불고기용 고기 한우에 관한 한우(韓牛)라는 말이 있습니다. 불고기용 고기 한우는 우리가 알고 있는 한우(韓牛)라는 뜻으로, 많은 지역에서 자라는 쇠고기의 한우를 일컫는 말입니다. 神文字 「自」(또는)입니다. 高書』입니다. 高 書 『神文字「自」입니다. 神文字「自」께서 불고기용 한우는 우리가 알고 있는 한우를 말하는 말입니다.\n",
      "Reference: 네, 저희 가게 불고기용 고기는 모두 최고급 한우만을 사용합니다. 맛과 품질을 보장합니다!\n",
      "  rouge1: Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "  rouge2: Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "  rougeL: Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "\n",
      "Prompt Question: 리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "Generated: 리처드 닉슨은 55대 부통령직을 수행했습니다. 1952년 12월 10일, 리처드 닉슨은 32대 부통령직을 수행했습니다. 1952년 1월 11일: 그는 부통령직을 수행하지 않았습니다. 필리프 닉슨은 50대 부통령직을 수행했습니다. 필리프 닉슨은 50대 부통령직을 수행했습니다. 필리프 닉슨은 35대 부통령직을 수행했습니다. 필리프 닉슨은 50대 부통령직을 수행했습니다. 필리프 닉슨은 35대 부통령직을 수행했습니다. 필리프 닉슨은 43대 부통령직을 수행했습니다. 필리프 닉슨은 53대 부통령직을 수행했습니다. 필리프 닉슨은 39대 부통령직을 수행했습니다. 필리프 닉슨은 41대 부통령직을 수행했습니다. 필리프 닉슨은 30대 부통령직을 수행했습니다. 필리프 닉슨은 50대 부통령직을 수행했습니다. 필리프 닉슨은 30대 부통령직을 수행했습니다. 필리프 닉슨은 30대 부통령직을 수행했습니다. 필리프 닉슨은 31대 부통령직을 수행했습니다.\n",
      "Reference: 리처드 닉슨은 1953년부터 1961년까지 미국의 제36대 부통령으로 재직했습니다. (주의: 43대 부통령은 댄 퀘일입니다. 질문의 의도에 따라 답변 수정 필요)\n",
      "  rouge1: Precision: 0.0455, Recall: 0.2500, F1-Score: 0.0769\n",
      "  rouge2: Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "  rougeL: Precision: 0.0455, Recall: 0.2500, F1-Score: 0.0769\n",
      "\n",
      "Prompt Question: 시카고 오헤어 국제공항은 어디에 있어\n",
      "Generated: 시카고 오헤어 국제공항은 미국 미네소타주에 위치해 있습니다.英審國際 국제공항이라고도 불립니다.英審國際 국제공항은 미국 미네소타주 시카고의 국제공항으로, 현재는 미국 중서부에 위치해 있습니다.英國際 국제공항은 미국 미네소타주의 세인트루이스 주에 위치해 있습니다.英國際 국제공항은 1869년 3월 15일에 설립되었습니다.英国際 국제공항은 미국 미네소타주 알론시아나에게 위치해 있으며, 현재는 국제공항으로 사용되고 있습니다.英國際 국제공항은 현재까지도 미국 중서부에서 매우 인기있는 도시 중 하나입니다.英国際 국제공항은 미국 중서부에서 매우 유명한 공항 중 하나로 여겨지며, 다른 지역의 공항이 될 수 있다는 것을 보여주고 있습니다.英国際 국제공항은 미국 중서부에 위치한 나라입니다.\n",
      "Reference: 시카고 오헤어 국제공항(O'Hare International Airport, IATA: ORD)은 미국 일리노이 주 시카고에 위치하고 있습니다.\n",
      "  rouge1: Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "  rouge2: Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "  rougeL: Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "\n",
      "Prompt Question: 오늘 미세먼지 어때?\n",
      "Generated: 미세먼지 정보를 확인하셔야 어떤 미세먼지 정보를 말씀하시는지 알려주시면 감사하겠습니다. 현재 진행중인 미세먼지 제거와 같은 활동을 적극적으로 활용하시기 바랍니다. 현재는 미세먼지 측정 시스템이 설치되어 있습니다. 자세한 정보가 필요하시면 전문가와 상담하시면 도움을 받을 수 있습니다. 상세한 정보를 얻을 수 있도록 최선의 노력을 하세요.箚直士와 결탁한 행위는 불건전한 행동 중 하나입니다.伺輩의 영향을 받아 미세먼지를 제거하고 다시 한번 노력하시겠다는 약속을 하셨습니다.前 戊辰國(황중)後文를 맡은 이력은 있으니, 참고 자료로 남겨두시길 바라며, 감사합니다\n",
      "Reference: 오늘 서울의 미세먼지 농도는 '보통' 수준입니다. 하지만 외출 시에는 마스크를 착용하시는 것이 좋겠습니다.\n",
      "  rouge1: Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "  rouge2: Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "  rougeL: Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "\n",
      "--- Average ROUGE F1-Scores ---\n",
      "Average rouge1 F1-Score: 0.0192\n",
      "Average rouge2 F1-Score: 0.0000\n",
      "Average rougeL F1-Score: 0.0192\n"
     ]
    }
   ],
   "source": [
    "import torch # 사용자 코드에 torch가 사용되어서 추가했습니다.\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ======================================================================\n",
    "# 사용자가 제공한 코드 시작\n",
    "# (tokenizer와 actor 모델은 이 코드가 실행되는 환경에 미리 로드되어 있어야 합니다)\n",
    "# 예시:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"your_tokenizer_model_name_or_path\")\n",
    "# actor = AutoModelForCausalLM.from_pretrained(\"your_ppo_model_name_or_path\").to(torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\")\n",
    "# ======================================================================\n",
    "\n",
    "def generation(input_text, tokenizer, actor_model): # tokenizer와 actor를 인자로 받도록 수정\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\" # GPU 사용 가능 여부 확인\n",
    "    )\n",
    "    # actor_model을 사용하여 생성하도록 수정\n",
    "    outputs = actor_model.generate(input_ids,\n",
    "                                 max_length=250,\n",
    "                                 do_sample=True,\n",
    "                                 top_k=50,\n",
    "                                 top_p=0.95,\n",
    "                                 num_return_sequences=1)\n",
    "    # outputs[0]이 아니라 outputs 자체를 넘기거나, 생성된 부분만 잘라내야 할 수 있습니다.\n",
    "    # 모델과 토크나이저에 따라 batch_decode 사용법이 다를 수 있으니 확인해주세요.\n",
    "    # 일반적으로 outputs 텐서 전체를 넣고, 각 시퀀스를 디코딩합니다.\n",
    "    # 여기서는 첫 번째 시퀀스만 사용한다고 가정합니다.\n",
    "    output_decoded_list = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    output = output_decoded_list[0] # 첫 번째 생성 결과 사용\n",
    "\n",
    "    # 프롬프트가 응답에 포함되어 나오는 경우가 많으므로, 프롬프트를 제외한 순수 응답 부분만 추출해야\n",
    "    # ROUGE 스코어 계산 시 더 정확할 수 있습니다.\n",
    "    # 예를 들어, 응답이 \"### Response(응답):실제 응답 내용\" 이렇다면 \"실제 응답 내용\"만 남기는 후처리가 필요합니다.\n",
    "    # PROMPT_DICT['prompt_input'].split(\"{prompt}\")[1] 등을 활용하여 응답 부분만 추출하는 로직을 추가할 수 있습니다.\n",
    "    # 아래는 간단한 예시 후처리입니다. (필요에 따라 수정/제거하세요)\n",
    "    response_marker = \"### Response(응답):\"\n",
    "    if response_marker in output:\n",
    "        output = output.split(response_marker)[-1].strip()\n",
    "\n",
    "    print(f\"Prompt: {input_text}\")\n",
    "    print(f\"Generated Output: {output}\\n\")\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt_questions = [ # 원본 질문 리스트\n",
    "    '불고기용 고기 한우에요?',\n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?'\n",
    "]\n",
    "\n",
    "# 프롬프트 형식에 맞게 변환\n",
    "list_formatted_prompts = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt_questions]\n",
    "\n",
    "# ======================================================================\n",
    "# 🎈 중요: 각 프롬프트에 대한 참조 답변을 여기에 작성해주세요! 🎈\n",
    "# list_formatted_prompts의 순서와 동일하게 각 질문에 대한 이상적인 답변을 넣어주세요.\n",
    "# ======================================================================\n",
    "reference_answers = [\n",
    "    \"네, 저희 가게 불고기용 고기는 모두 최고급 한우만을 사용합니다. 맛과 품질을 보장합니다!\", # '불고기용 고기 한우에요?'에 대한 답변 예시\n",
    "    \"리처드 닉슨은 1953년부터 1961년까지 미국의 제36대 부통령으로 재직했습니다. (주의: 43대 부통령은 댄 퀘일입니다. 질문의 의도에 따라 답변 수정 필요)\", # '리처드 닉슨이 43대 부통령직을 수행한 년도는?'에 대한 답변 예시\n",
    "    \"시카고 오헤어 국제공항(O'Hare International Airport, IATA: ORD)은 미국 일리노이 주 시카고에 위치하고 있습니다.\", # '시카고 오헤어 국제공항은 어디에 있어'에 대한 답변 예시\n",
    "    \"오늘 서울의 미세먼지 농도는 '보통' 수준입니다. 하지만 외출 시에는 마스크를 착용하시는 것이 좋겠습니다.\" # '오늘 미세먼지 어때?'에 대한 답변 예시 (실제 값은 매일 달라지므로 예시)\n",
    "]\n",
    "\n",
    "# 생성된 답변들을 저장할 리스트\n",
    "generated_answers = []\n",
    "\n",
    "# ======================================================================\n",
    "# 중요: 실제 tokenizer와 actor 모델을 로드하는 코드가 이 부분 이전에 필요합니다.\n",
    "# 아래는 예시이며, 실제 모델 경로와 클래스를 사용해야 합니다.\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") # 예시 토크나이저\n",
    "# actor = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\") # 예시 모델\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "# 각 프롬프트에 대해 답변 생성\n",
    "# 이 부분은 실제 모델과 토크나이저가 로드된 후에 실행해야 합니다.\n",
    "# 지금은 주석 처리하며, 실제 실행 시 주석을 해제하고 tokenizer와 actor를 전달해주세요.\n",
    "# for input_text in list_formatted_prompts:\n",
    "# output = generation(input_text, tokenizer, actor) # 수정된 generation 함수 호출\n",
    "# generated_answers.append(output)\n",
    "\n",
    "\n",
    "# --- ROUGE 스코어 계산 ---\n",
    "# 만약 위에서 답변 생성을 주석 처리했다면, 아래 generated_answers는 수동으로 채워야 합니다.\n",
    "# 예시: generated_answers = [\"모델 답변1\", \"모델 답변2\", ...] (실제 모델 출력을 가져와야 함)\n",
    "\n",
    "# ROUGE 스코어 계산기 초기화\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "all_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "print(\"\\n--- ROUGE Score Results ---\")\n",
    "if not generated_answers:\n",
    "    print(\"⚠️ 생성된 답변이 없습니다. `generation` 함수를 호출하여 `generated_answers` 리스트를 채워주세요.\")\n",
    "    print(\"   또는, 수동으로 `generated_answers` 리스트에 PPO 모델의 답변을 넣어주세요.\")\n",
    "    generated_answers = [\n",
    "    \"죄송합니다, 제가 인공지능 챗봇이므로 이에 대한 정보를 알지 못합니다. 하지만 불고기가 어느 지역인지 알려주시면 더 자세한 정보를 알려드리도록 하겠습니다. 감사합니다. 神文字「宗書」입니다. 臣衆書 『東]입니다. 불고기용 고기 한우에 관한 한우(韓牛)라는 말이 있습니다. 불고기용 고기 한우는 우리가 알고 있는 한우(韓牛)라는 뜻으로, 많은 지역에서 자라는 쇠고기의 한우를 일컫는 말입니다. 神文字 「自」(또는)입니다. 高書』입니다. 高 書 『神文字「自」입니다. 神文字「自」께서 불고기용 한우는 우리가 알고 있는 한우를 말하는 말입니다.\",\n",
    "    \"리처드 닉슨은 55대 부통령직을 수행했습니다. 1952년 12월 10일, 리처드 닉슨은 32대 부통령직을 수행했습니다. 1952년 1월 11일: 그는 부통령직을 수행하지 않았습니다. 필리프 닉슨은 50대 부통령직을 수행했습니다. 필리프 닉슨은 50대 부통령직을 수행했습니다. 필리프 닉슨은 35대 부통령직을 수행했습니다. 필리프 닉슨은 50대 부통령직을 수행했습니다. 필리프 닉슨은 35대 부통령직을 수행했습니다. 필리프 닉슨은 43대 부통령직을 수행했습니다. 필리프 닉슨은 53대 부통령직을 수행했습니다. 필리프 닉슨은 39대 부통령직을 수행했습니다. 필리프 닉슨은 41대 부통령직을 수행했습니다. 필리프 닉슨은 30대 부통령직을 수행했습니다. 필리프 닉슨은 50대 부통령직을 수행했습니다. 필리프 닉슨은 30대 부통령직을 수행했습니다. 필리프 닉슨은 30대 부통령직을 수행했습니다. 필리프 닉슨은 31대 부통령직을 수행했습니다.\",\n",
    "    \"시카고 오헤어 국제공항은 미국 미네소타주에 위치해 있습니다.英審國際 국제공항이라고도 불립니다.英審國際 국제공항은 미국 미네소타주 시카고의 국제공항으로, 현재는 미국 중서부에 위치해 있습니다.英國際 국제공항은 미국 미네소타주의 세인트루이스 주에 위치해 있습니다.英國際 국제공항은 1869년 3월 15일에 설립되었습니다.英国際 국제공항은 미국 미네소타주 알론시아나에게 위치해 있으며, 현재는 국제공항으로 사용되고 있습니다.英國際 국제공항은 현재까지도 미국 중서부에서 매우 인기있는 도시 중 하나입니다.英国際 국제공항은 미국 중서부에서 매우 유명한 공항 중 하나로 여겨지며, 다른 지역의 공항이 될 수 있다는 것을 보여주고 있습니다.英国際 국제공항은 미국 중서부에 위치한 나라입니다.\",\n",
    "    \"미세먼지 정보를 확인하셔야 어떤 미세먼지 정보를 말씀하시는지 알려주시면 감사하겠습니다. 현재 진행중인 미세먼지 제거와 같은 활동을 적극적으로 활용하시기 바랍니다. 현재는 미세먼지 측정 시스템이 설치되어 있습니다. 자세한 정보가 필요하시면 전문가와 상담하시면 도움을 받을 수 있습니다. 상세한 정보를 얻을 수 있도록 최선의 노력을 하세요.箚直士와 결탁한 행위는 불건전한 행동 중 하나입니다.伺輩의 영향을 받아 미세먼지를 제거하고 다시 한번 노력하시겠다는 약속을 하셨습니다.前 戊辰國(황중)後文를 맡은 이력은 있으니, 참고 자료로 남겨두시길 바라며, 감사합니다\"\n",
    "    ]\n",
    "    if not generated_answers: # 그래도 비어있으면 실행 중단\n",
    "        exit()\n",
    "\n",
    "\n",
    "if len(generated_answers) != len(reference_answers):\n",
    "    print(f\"⚠️ 생성된 답변의 수({len(generated_answers)})와 참조 답변의 수({len(reference_answers)})가 일치하지 않습니다!\")\n",
    "else:\n",
    "    for i in range(len(generated_answers)):\n",
    "        gen_text = generated_answers[i]\n",
    "        ref_text = reference_answers[i]\n",
    "\n",
    "        print(f\"\\nPrompt Question: {list_prompt_questions[i]}\")\n",
    "        print(f\"Generated: {gen_text}\")\n",
    "        print(f\"Reference: {ref_text}\")\n",
    "\n",
    "        scores = scorer.score(ref_text, gen_text)\n",
    "        for key in scores:\n",
    "            print(f'  {key}: Precision: {scores[key].precision:.4f}, Recall: {scores[key].recall:.4f}, F1-Score: {scores[key].fmeasure:.4f}')\n",
    "            all_scores[key].append(scores[key].fmeasure)\n",
    "\n",
    "    print(\"\\n--- Average ROUGE F1-Scores ---\")\n",
    "    for key in all_scores:\n",
    "        if all_scores[key]: # 점수가 하나라도 있을 경우에만 평균 계산\n",
    "            average_f1 = sum(all_scores[key]) / len(all_scores[key])\n",
    "            print(f'Average {key} F1-Score: {average_f1:.4f}')\n",
    "        else:\n",
    "            print(f'Average {key} F1-Score: N/A (no scores to average)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee1c96b",
   "metadata": {},
   "source": [
    "| ROUGE 지표 | SFT 모델 (평균 F1-Score) | PPO 모델 (평균 F1-Score) |\n",
    "| :--------- | :---------------------- | :---------------------- |\n",
    "| **ROUGE1** | 0.0625                  | 0.0192                  |\n",
    "| **ROUGE2** | 0.0000                  | 0.0000                  |\n",
    "| **ROUGEL** | 0.0625                  | 0.0192                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f01d78",
   "metadata": {},
   "source": [
    "## PPO - ROUGE 스코어 평가\n",
    "\n",
    "PPO모델보다 SFT모델이 더 좋은 성능을 보이고 있다.\n",
    "\n",
    "정성평가 정량평가가 어느정도 일치한다는것을 확인할 수 있었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c9817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f12b998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 설정\n",
    "epsilon = 0.2\n",
    "advantage = 1.0  # 양수인 경우\n",
    "\n",
    "# r_t: 새로운 정책이 이전보다 얼마나 좋아졌는지 나타내는 비율\n",
    "r = np.linspace(0, 2, 100)\n",
    "\n",
    "# unclipped objective: r * A\n",
    "unclipped = r * advantage\n",
    "\n",
    "# clipped objective\n",
    "clipped = np.clip(r, 1 - epsilon, 1 + epsilon) * advantage\n",
    "\n",
    "# 최종 PPO objective: 둘 중 최소값 선택\n",
    "ppo_objective = np.minimum(unclipped, clipped)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(r, unclipped, label='Unclipped Objective (r*A)', linestyle='--')\n",
    "plt.plot(r, clipped, label='Clipped (r clipped to [1-ε, 1+ε])', linestyle=':')\n",
    "plt.plot(r, ppo_objective, label='PPO Final Objective', linewidth=2)\n",
    "plt.axvline(1 - epsilon, color='gray', linestyle=':', alpha=0.7)\n",
    "plt.axvline(1 + epsilon, color='gray', linestyle=':', alpha=0.7)\n",
    "plt.title(\"PPO Clipped Objective Visualization (Advantage > 0)\")\n",
    "plt.xlabel(\"Probability Ratio r = π_θ(a|s) / π_old(a|s)\")\n",
    "plt.ylabel(\"Objective Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ffc9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
